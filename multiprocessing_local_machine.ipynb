{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU only without parallel computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(json_data):\n",
    "    # Parse JSON data\n",
    "    candidates = json_data['long_answer_candidates']\n",
    "    doc_tokenized = json_data['document_text'].split(' ')\n",
    "    question = json_data['question_text']\n",
    "    question_s = question.split(' ') \n",
    "    annotation = json_data['annotations'][0]\n",
    "\n",
    "    # TFIDF for the document\n",
    "    # Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words)\n",
    "    tfidf.fit([json_data['document_text']])  \n",
    "    q_tfidf = tfidf.transform([question]).todense() \n",
    "    \n",
    "    # Find the nearest answer from candidates using cosine distanse\n",
    "    scores = []\n",
    "    for i, c in enumerate(candidates):\n",
    "        s, e = c['start_token'], c['end_token']\n",
    "        t = ' '.join(doc_tokenized[s:e])\n",
    "        t_tfidf = tfidf.transform([t]).todense()\n",
    "       \n",
    "        score = 1 - spatial.distance.cosine(q_tfidf, t_tfidf)\n",
    "        scores.append(score)\n",
    "\n",
    "    # Put the nearest condidate \n",
    "\n",
    "    ans = (np.array(candidates)[np.argsort(scores)])[-1] # dict, top condidate\n",
    "    \n",
    "    if np.max(scores) < 0.2:\n",
    "        ans_long = '-1:-1'\n",
    "        ans = {'start_token': 0, 'end_token': 0}\n",
    "    else:\n",
    "        ans_long = str(ans['start_token']) + ':' + str(ans['end_token'])\n",
    "              \n",
    "    return ans_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe004e8e2cad4195bc05e0a45bb492ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.1013\n",
      "CPU times: user 13min 28s, sys: 1.28 s, total: 13min 29s\n",
      "Wall time: 13min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ids, annotations, predictions = [], [], []\n",
    "n_samples = 10000\n",
    "with open('data/10k.json', 'r') as json_file:\n",
    "    cnt = 0\n",
    "    for line in tqdm(json_file):\n",
    "        json_data = json.loads(line)\n",
    "\n",
    "        annotated_answer = str(json_data['annotations'][0]['long_answer']['start_token']) + ':' + \\\n",
    "            str(json_data['annotations'][0]['long_answer']['end_token'])\n",
    "        \n",
    "        predicted_answer = predict(json_data)\n",
    "        \n",
    "        ids.append(str(json_data['example_id']) + '_long')\n",
    "        annotations.append(annotated_answer)\n",
    "        predictions.append(predicted_answer)\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt >= n_samples:\n",
    "            break\n",
    "\n",
    "# Generating Dataframe\n",
    "df = pd.DataFrame()\n",
    "df['example_id'] = ids\n",
    "df['CorrectString'] = annotations\n",
    "df['PredictionString'] = predictions\n",
    "\n",
    "# Evaluating\n",
    "f1 = f1_score(df['CorrectString'].values, df['PredictionString'].values, average='micro')\n",
    "print(f'F1-score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import Process, Manager\n",
    "import multiprocessing\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logical CPU count: 8\n",
      " Physical CPU count: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\" Logical CPU count: {psutil.cpu_count(logical=True)}\")\n",
    "print(f\" Physical CPU count: {psutil.cpu_count(logical=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(json_path, chunk_index, total_list):\n",
    "    \n",
    "    ids, annotations, predictions = [], [], []\n",
    "    n_rows = 10000\n",
    "    num_cores = 4\n",
    "    chunk_size = int(n_rows/num_cores)  # number of rows for 1 chunk\n",
    "    \n",
    "    with open(json_path, 'r') as json_file:\n",
    "        \n",
    "        cnt = 0 + (chunk_index-1)*chunk_size # starting row\n",
    "        start_row = cnt\n",
    "        finish_row = chunk_size*chunk_index\n",
    "        \n",
    "        for i, line in enumerate(json_file):\n",
    "           \n",
    "            if i < start_row or i > finish_row:\n",
    "                continue\n",
    "            \n",
    "            json_data = json.loads(line)\n",
    "            annotated_answer = str(json_data['annotations'][0]['long_answer']['start_token']) + ':' + \\\n",
    "                str(json_data['annotations'][0]['long_answer']['end_token'])\n",
    "\n",
    "            predicted_answer = predict(json_data)\n",
    "\n",
    "            ids.append(str(json_data['example_id']) + '_long')\n",
    "            annotations.append(annotated_answer)\n",
    "            predictions.append(predicted_answer)\n",
    "\n",
    "            cnt += 1\n",
    "            \n",
    "            if cnt%(chunk_size/10) == 0 and cnt < (chunk_size+1):\n",
    "                print(f\"computing progress: {int(cnt*100/chunk_size)}%\")\n",
    "            \n",
    "            if cnt >= finish_row:\n",
    "                break\n",
    "\n",
    "    chunk_dict = {}\n",
    "    chunk_dict['example_id'] = ids\n",
    "    chunk_dict['CorrectString'] = annotations\n",
    "    chunk_dict['PredictionString'] = predictions\n",
    "    total_list.append(chunk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_list = list()\n",
    "def multiprocessed():\n",
    "    cores = 4\n",
    "    processes = []\n",
    "    a = time.time()\n",
    "    with Manager() as manager:\n",
    "        sum_list = manager.list()  # <-- can be shared between processes.\n",
    "        for i in range(0, cores):\n",
    "            p = Process(target=process,args=('data/10k.json', i+1, sum_list))\n",
    "            processes.append(p)\n",
    "        # Start the processes\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "        # Ensure all processes have finished execution\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        sum_list = list(sum_list)\n",
    "        b = time.time()\n",
    "        print(f\"the executing time using multiprocessing is: {round(b-a, 3)} sec\")\n",
    "        return sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing progress: 10%\n",
      "computing progress: 20%\n",
      "computing progress: 30%\n",
      "computing progress: 40%\n",
      "computing progress: 50%\n",
      "computing progress: 60%\n",
      "computing progress: 70%\n",
      "computing progress: 80%\n",
      "computing progress: 90%\n",
      "computing progress: 100%\n",
      "the executing time using multiprocessing is: 245.106 sec\n"
     ]
    }
   ],
   "source": [
    "sum_list = multiprocessed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.1013\n"
     ]
    }
   ],
   "source": [
    "def creating_df(lst):\n",
    "    total_df = pd.DataFrame()\n",
    "    for l in lst:\n",
    "        df_chunk = pd.DataFrame.from_dict(l)\n",
    "        total_df = total_df.append(df_chunk)\n",
    "    total_df.reset_index(inplace=True, drop=True)\n",
    "    return total_df\n",
    "total_df = creating_df(sum_list)    \n",
    "f1 = f1_score(total_df['CorrectString'].values, total_df['PredictionString'].values, average='micro')\n",
    "print(f'F1-score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
